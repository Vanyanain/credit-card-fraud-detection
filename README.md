# ğŸ’³ Credit Card Fraud Detection  
## End-to-End Machine Learning + Transformer-Based System

---

## ğŸ“Œ Overview

This project builds a complete end-to-end fraud detection system to classify credit card transactions as **fraudulent** or **legitimate**.

To solve this highly imbalanced real-world financial problem, multiple approaches were implemented and compared:

- Traditional Machine Learning models  
- Gradient Boosting (LightGBM)  
- Transformer-based Deep Learning architecture  

The objective was to maximize fraud detection performance while minimizing false negatives.

---

## ğŸ“Š Dataset

- **Total transactions:** 284,807  
- **Fraudulent transactions:** 492  
- **Fraud rate:** 0.172%  
- **Highly imbalanced dataset**

The dataset contains only numerical features obtained through PCA transformation due to confidentiality constraints.

**Target variable:**
- `0` â†’ Legitimate transaction  
- `1` â†’ Fraudulent transaction  

---

## ğŸ” Problem Challenge

Because fraud accounts for only 0.17% of transactions:

- Accuracy becomes misleading  
- Models tend to predict the majority class  
- False negatives are extremely costly  

To address this, **Random Undersampling** was applied to rebalance the dataset.
from imblearn.under_sampling import RandomUnderSampler

undersample = RandomUnderSampler(sampling_strategy=0.5)
## ğŸ§  Algorithms Implemented

This project compares classical ML, boosting techniques, and deep learning architectures.

###ğŸ”¹ Traditional Machine Learning Models
â€¢ Logistic Regression
Baseline linear classifier
Improved performance after feature scaling

â€¢ Support Vector Machine (SVM)
Effective in high-dimensional feature space
Captures complex decision boundaries

â€¢ Decision Tree
Handles non-linear relationships
Interpretable model structure

â€¢ Random Forest (Bagging)
Ensemble learning method
Reduces overfitting
Strong performance on structured tabular data

###ğŸ”¹ Gradient Boosting Model
â€¢ LightGBM (Light Gradient Boosting Machine)
Fast training speed
Handles large datasets efficiently
Excellent performance on imbalanced data
Leaf-wise tree growth strategy
Memory efficient and highly optimized
LightGBM significantly improved predictive performance compared to basic tree models.

###ğŸ”¹ Advanced Deep Learning Model
â€¢ Transformer-Based Neural Network
Implemented a Transformer architecture adapted for tabular fraud detection.
Key Components:
Multi-Head Self-Attention
Positional Encoding
Feed-Forward Layers
Layer Normalization
Dropout Regularization
The Transformer captures complex feature interactions and enhances fraud pattern recognition through attention mechanisms.

##âš™ï¸ Data Engineering & Preprocessing
âœ” Removed non-informative features
âœ” Standardized transaction amount using StandardScaler
âœ” Engineered scaled feature (std_Amount)
âœ” Visualized fraud distribution
âœ” Applied Random Undersampling
âœ” Train-Test split (80/20)

##ğŸ“Š Model Evaluation Strategy
Due to extreme class imbalance, the following metrics were prioritized:
Recall (Primary metric â€“ Fraud Detection Rate)
Precision
F1 Score
ROC-AUC Score
Precision-Recall Curve
Confusion Matrix
Accuracy alone was not used as a primary evaluation metric.

##ğŸ“‚ End-to-End Pipeline
Data Loading
â†’ Data Cleaning
â†’ Feature Scaling
â†’ Handling Class Imbalance
â†’ Train-Test Split
â†’ Model Training (ML + LightGBM + Transformer)
â†’ Model Evaluation
â†’ ROC & PR Curve Analysis
â†’ Model Persistence

##ğŸ“ˆ Why This Project Stands Out
âœ” Solves a real-world financial risk problem
âœ” Handles extreme class imbalance properly
âœ” Compares multiple ML paradigms
âœ” Implements boosting + attention-based deep learning
âœ” Uses correct evaluation metrics beyond accuracy
âœ” Deployment-ready model saving

##ğŸ› ï¸ Tech Stack
Python
Pandas & NumPy
Scikit-learn
LightGBM
XGBoost
TensorFlow / Keras
Imbalanced-learn
Matplotlib & Seaborn
Joblib

##ğŸ“ˆ Core Learning Outcomes
Handling highly imbalanced datasets
Importance of Recall in fraud detection
Comparative model performance analysis
Boosting vs Bagging techniques
Attention mechanisms in tabular data
Model interpretability & evaluation

## Author
Vanya Nain
Machine Learning | Deep Learning | AI Enthusiast
